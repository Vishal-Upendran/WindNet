{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the activation maps\n",
    "\n",
    "This script generates the required activation maps. I have multiple python scripts, one to load the model and data, and one to perform the perform the average maps. I have been unable to share environments and make this into a single bash or python script, so Jupyter it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#import tensorflow as tf \n",
    "import numpy as np \n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "#import seaborn as sns\n",
    "from glob import glob\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Keys must be of the form attention_type_area_history_delay, where:\\n    type: 'Total', 'Large', or 'Small' for corresponding wind speeds\\n    area: 'hole' or 'hot'\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WindNet_attention={}\n",
    "'''\n",
    "    Keys must be of the form attention_type_area_history_delay, where:\n",
    "    type: 'Total', 'Large', or 'Small' for corresponding wind speeds\n",
    "    area: 'hole' or 'hot'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = np.zeros([224,224])\n",
    "for i in xrange(224):\n",
    "    for j in xrange(224):\n",
    "        if np.square(i-112)+np.square(j-112)<=90*90:\n",
    "            disc[i,j]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hole', 'hot']\n",
      "(20, 224, 224)\n",
      "(224, 224)\n",
      "0.00021217907869259693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7213934290>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp=np.load('../Data/Mask/CrossValidation211/CV_1/Test/Partition103.npy',allow_pickle=True).tolist()\n",
    "print tmp.keys()\n",
    "print tmp['hot'].shape\n",
    "#plt.imshow(tmp['hole'][0,:,:],cmap='hot')\n",
    "t2=tmp['hole'][0,:]+tmp['hot'][0,:]\n",
    "t2=t2*disc\n",
    "print t2.shape\n",
    "print np.max(tmp['hot'][0,:])\n",
    "plt.imshow(t2,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../Data/CrossValidation193/CV_1/Test/Partition1.npy', '../Data/CrossValidation193/CV_1/Test/Partition100.npy', '../Data/CrossValidation193/CV_1/Test/Partition128.npy', '../Data/CrossValidation193/CV_1/Test/Partition129.npy', '../Data/CrossValidation193/CV_1/Test/Partition131.npy', '../Data/CrossValidation193/CV_1/Test/Partition136.npy', '../Data/CrossValidation193/CV_1/Test/Partition137.npy', '../Data/CrossValidation193/CV_1/Test/Partition139.npy', '../Data/CrossValidation193/CV_1/Test/Partition141.npy', '../Data/CrossValidation193/CV_1/Test/Partition142.npy', '../Data/CrossValidation193/CV_1/Test/Partition155.npy', '../Data/CrossValidation193/CV_1/Test/Partition18.npy', '../Data/CrossValidation193/CV_1/Test/Partition20.npy', '../Data/CrossValidation193/CV_1/Test/Partition28.npy', '../Data/CrossValidation193/CV_1/Test/Partition30.npy', '../Data/CrossValidation193/CV_1/Test/Partition31.npy', '../Data/CrossValidation193/CV_1/Test/Partition32.npy', '../Data/CrossValidation193/CV_1/Test/Partition33.npy', '../Data/CrossValidation193/CV_1/Test/Partition34.npy', '../Data/CrossValidation193/CV_1/Test/Partition35.npy', '../Data/CrossValidation193/CV_1/Test/Partition36.npy', '../Data/CrossValidation193/CV_1/Test/Partition38.npy', '../Data/CrossValidation193/CV_1/Test/Partition43.npy', '../Data/CrossValidation193/CV_1/Test/Partition49.npy', '../Data/CrossValidation193/CV_1/Test/Partition58.npy', '../Data/CrossValidation193/CV_1/Test/Partition59.npy', '../Data/CrossValidation193/CV_1/Test/Partition65.npy', '../Data/CrossValidation193/CV_1/Test/Partition81.npy', '../Data/CrossValidation193/CV_1/Test/Partition82.npy', '../Data/CrossValidation193/CV_1/Test/Partition93.npy', '../Data/CrossValidation193/CV_1/Test/Partition95.npy']\n",
      "['../Data/Mask/CrossValidation193/CV_1/Test/Partition1.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition100.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition128.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition129.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition131.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition136.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition137.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition139.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition141.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition142.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition155.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition18.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition20.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition28.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition30.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition31.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition32.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition33.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition34.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition35.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition36.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition38.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition43.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition49.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition58.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition59.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition65.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition81.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition82.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition93.npy', '../Data/Mask/CrossValidation193/CV_1/Test/Partition95.npy']\n"
     ]
    }
   ],
   "source": [
    "print sorted(glob('../Data/CrossValidation193/CV_1/Test/*.npy'))\n",
    "print sorted(glob('../Data/Mask/CrossValidation193/CV_1/Test/*.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2413, 1, 1, 224, 224)\n",
      "(2413, 1, 224, 224, 1)\n",
      "(73, 1, 1) (73, 1, 1)\n",
      "(184, 1, 1) (184, 1, 1)\n",
      "(2356, 1, 1, 224, 224)\n",
      "(2356, 1, 224, 224, 1)\n",
      "(127, 1, 1) (127, 1, 1)\n",
      "(320, 1, 1) (320, 1, 1)\n",
      "(2470, 1, 1, 224, 224)\n",
      "(2470, 1, 224, 224, 1)\n",
      "(68, 1, 1) (68, 1, 1)\n",
      "(339, 1, 1) (339, 1, 1)\n",
      "(2546, 1, 1, 224, 224)\n",
      "(2546, 1, 224, 224, 1)\n",
      "(57, 1, 1) (57, 1, 1)\n",
      "(183, 1, 1) (183, 1, 1)\n",
      "(2223, 1, 1, 224, 224)\n",
      "(2223, 1, 224, 224, 1)\n",
      "(24, 1, 1) (24, 1, 1)\n",
      "(564, 1, 1) (564, 1, 1)\n",
      "(2286, 1, 1, 224, 224)\n",
      "(2286, 1, 224, 224, 1)\n",
      "(74, 1, 1) (74, 1, 1)\n",
      "(176, 1, 1) (176, 1, 1)\n",
      "(2232, 1, 1, 224, 224)\n",
      "(2232, 1, 224, 224, 1)\n",
      "(63, 1, 1) (63, 1, 1)\n",
      "(142, 1, 1) (142, 1, 1)\n",
      "(2340, 1, 1, 224, 224)\n",
      "(2340, 1, 224, 224, 1)\n",
      "(66, 1, 1) (66, 1, 1)\n",
      "(378, 1, 1) (378, 1, 1)\n",
      "(2412, 1, 1, 224, 224)\n",
      "(2412, 1, 224, 224, 1)\n",
      "(71, 1, 1) (71, 1, 1)\n",
      "(312, 1, 1) (312, 1, 1)\n",
      "(2106, 1, 1, 224, 224)\n",
      "(2106, 1, 224, 224, 1)\n",
      "(0, 1, 1) (0, 1, 1)\n",
      "(482, 1, 1) (482, 1, 1)\n",
      "(2159, 1, 1, 224, 224)\n",
      "(2159, 1, 224, 224, 1)\n",
      "(211, 1, 1) (211, 1, 1)\n",
      "(177, 1, 1) (177, 1, 1)\n",
      "(2108, 1, 1, 224, 224)\n",
      "(2108, 1, 224, 224, 1)\n",
      "(108, 1, 1) (108, 1, 1)\n",
      "(390, 1, 1) (390, 1, 1)\n",
      "(2210, 1, 1, 224, 224)\n",
      "(2210, 1, 224, 224, 1)\n",
      "(133, 1, 1) (133, 1, 1)\n",
      "(448, 1, 1) (448, 1, 1)\n",
      "(2278, 1, 1, 224, 224)\n",
      "(2278, 1, 224, 224, 1)\n",
      "(112, 1, 1) (112, 1, 1)\n",
      "(266, 1, 1) (266, 1, 1)\n",
      "(1989, 1, 1, 224, 224)\n",
      "(1989, 1, 224, 224, 1)\n",
      "(96, 1, 1) (96, 1, 1)\n",
      "(451, 1, 1) (451, 1, 1)\n",
      "(2032, 1, 1, 224, 224)\n",
      "(2032, 1, 224, 224, 1)\n",
      "(114, 1, 1) (114, 1, 1)\n",
      "(182, 1, 1) (182, 1, 1)\n",
      "(1984, 1, 1, 224, 224)\n",
      "(1984, 1, 224, 224, 1)\n",
      "(171, 1, 1) (171, 1, 1)\n",
      "(243, 1, 1) (243, 1, 1)\n",
      "(2080, 1, 1, 224, 224)\n",
      "(2080, 1, 224, 224, 1)\n",
      "(170, 1, 1) (170, 1, 1)\n",
      "(321, 1, 1) (321, 1, 1)\n",
      "(2144, 1, 1, 224, 224)\n",
      "(2144, 1, 224, 224, 1)\n",
      "(136, 1, 1) (136, 1, 1)\n",
      "(232, 1, 1) (232, 1, 1)\n",
      "(1872, 1, 1, 224, 224)\n",
      "(1872, 1, 224, 224, 1)\n",
      "(25, 1, 1) (25, 1, 1)\n",
      "(451, 1, 1) (451, 1, 1)\n",
      "(2286, 1, 2, 224, 224)\n",
      "(2286, 2, 224, 224, 1)\n",
      "(395, 2, 1) (395, 2, 1)\n",
      "(430, 2, 1) (430, 2, 1)\n",
      "(2232, 1, 2, 224, 224)\n",
      "(2232, 2, 224, 224, 1)\n",
      "(611, 2, 1) (611, 2, 1)\n",
      "(239, 2, 1) (239, 2, 1)\n",
      "(2340, 1, 2, 224, 224)\n",
      "(2340, 2, 224, 224, 1)\n",
      "(199, 2, 1) (199, 2, 1)\n",
      "(425, 2, 1) (425, 2, 1)\n",
      "(2412, 1, 2, 224, 224)\n",
      "(2412, 2, 224, 224, 1)\n",
      "(67, 2, 1) (67, 2, 1)\n",
      "(405, 2, 1) (405, 2, 1)\n",
      "(2106, 1, 2, 224, 224)\n",
      "(2106, 2, 224, 224, 1)\n",
      "(163, 2, 1) (163, 2, 1)\n",
      "(354, 2, 1) (354, 2, 1)\n",
      "(2159, 1, 2, 224, 224)\n",
      "(2159, 2, 224, 224, 1)\n",
      "(727, 2, 1) (727, 2, 1)\n",
      "(208, 2, 1) (208, 2, 1)\n",
      "(2108, 1, 2, 224, 224)\n",
      "(2108, 2, 224, 224, 1)\n",
      "(288, 2, 1) (288, 2, 1)\n",
      "(523, 2, 1) (523, 2, 1)\n",
      "(2210, 1, 2, 224, 224)\n",
      "(2210, 2, 224, 224, 1)\n",
      "(158, 2, 1) (158, 2, 1)\n",
      "(424, 2, 1) (424, 2, 1)\n",
      "(2278, 1, 2, 224, 224)\n",
      "(2278, 2, 224, 224, 1)\n",
      "(249, 2, 1) (249, 2, 1)\n",
      "(714, 2, 1) (714, 2, 1)\n",
      "(1989, 1, 2, 224, 224)\n",
      "(1989, 2, 224, 224, 1)\n",
      "(193, 2, 1) (193, 2, 1)\n",
      "(370, 2, 1) (370, 2, 1)\n",
      "(2032, 1, 2, 224, 224)\n",
      "(2032, 2, 224, 224, 1)\n",
      "(307, 2, 1) (307, 2, 1)\n",
      "(210, 2, 1) (210, 2, 1)\n",
      "(1984, 1, 2, 224, 224)\n",
      "(1984, 2, 224, 224, 1)\n",
      "(118, 2, 1) (118, 2, 1)\n",
      "(407, 2, 1) (407, 2, 1)\n",
      "(2080, 1, 2, 224, 224)\n",
      "(2080, 2, 224, 224, 1)\n",
      "(170, 2, 1) (170, 2, 1)\n",
      "(605, 2, 1) (605, 2, 1)\n",
      "(2144, 1, 2, 224, 224)\n",
      "(2144, 2, 224, 224, 1)\n",
      "(345, 2, 1) (345, 2, 1)\n",
      "(331, 2, 1) (331, 2, 1)\n",
      "(1872, 1, 2, 224, 224)\n",
      "(1872, 2, 224, 224, 1)\n",
      "(35, 2, 1) (35, 2, 1)\n",
      "(571, 2, 1) (571, 2, 1)\n",
      "(1905, 1, 2, 224, 224)\n",
      "(1905, 2, 224, 224, 1)\n",
      "(534, 2, 1) (534, 2, 1)\n",
      "(121, 2, 1) (121, 2, 1)\n",
      "(1860, 1, 2, 224, 224)\n",
      "(1860, 2, 224, 224, 1)\n",
      "(199, 2, 1) (199, 2, 1)\n",
      "(229, 2, 1) (229, 2, 1)\n",
      "(1950, 1, 2, 224, 224)\n",
      "(1950, 2, 224, 224, 1)\n",
      "(97, 2, 1) (97, 2, 1)\n",
      "(587, 2, 1) (587, 2, 1)\n",
      "(2010, 1, 2, 224, 224)\n",
      "(2010, 2, 224, 224, 1)\n",
      "(178, 2, 1) (178, 2, 1)\n",
      "(261, 2, 1) (261, 2, 1)\n",
      "(1755, 1, 2, 224, 224)\n",
      "(1755, 2, 224, 224, 1)\n",
      "(168, 2, 1) (168, 2, 1)\n",
      "(277, 2, 1) (277, 2, 1)\n",
      "(2159, 1, 3, 224, 224)\n",
      "(2159, 3, 224, 224, 1)\n",
      "(415, 3, 1) (415, 3, 1)\n",
      "(196, 3, 1) (196, 3, 1)\n",
      "(2108, 1, 3, 224, 224)\n",
      "(2108, 3, 224, 224, 1)\n",
      "(261, 3, 1) (261, 3, 1)\n",
      "(517, 3, 1) (517, 3, 1)\n",
      "(2210, 1, 3, 224, 224)\n",
      "(2210, 3, 224, 224, 1)\n",
      "(133, 3, 1) (133, 3, 1)\n",
      "(498, 3, 1) (498, 3, 1)\n",
      "(2278, 1, 3, 224, 224)\n",
      "(2278, 3, 224, 224, 1)\n",
      "(665, 3, 1) (665, 3, 1)\n",
      "(292, 3, 1) (292, 3, 1)\n",
      "(1989, 1, 3, 224, 224)\n",
      "(1989, 3, 224, 224, 1)\n",
      "(257, 3, 1) (257, 3, 1)\n",
      "(356, 3, 1) (356, 3, 1)\n",
      "(2032, 1, 3, 224, 224)\n",
      "(2032, 3, 224, 224, 1)\n",
      "(384, 3, 1) (384, 3, 1)\n",
      "(371, 3, 1) (371, 3, 1)\n",
      "(1984, 1, 3, 224, 224)\n",
      "(1984, 3, 224, 224, 1)\n",
      "(261, 3, 1) (261, 3, 1)\n",
      "(442, 3, 1) (442, 3, 1)\n",
      "(2080, 1, 3, 224, 224)\n",
      "(2080, 3, 224, 224, 1)\n",
      "(112, 3, 1) (112, 3, 1)\n",
      "(324, 3, 1) (324, 3, 1)\n",
      "(2144, 1, 3, 224, 224)\n",
      "(2144, 3, 224, 224, 1)\n",
      "(104, 3, 1) (104, 3, 1)\n",
      "(991, 3, 1) (991, 3, 1)\n",
      "(1872, 1, 3, 224, 224)\n",
      "(1872, 3, 224, 224, 1)\n",
      "(245, 3, 1) (245, 3, 1)\n",
      "(523, 3, 1) (523, 3, 1)\n",
      "(1905, 1, 3, 224, 224)\n",
      "(1905, 3, 224, 224, 1)\n",
      "(477, 3, 1) (477, 3, 1)\n",
      "(256, 3, 1) (256, 3, 1)\n",
      "(1860, 1, 3, 224, 224)\n",
      "(1860, 3, 224, 224, 1)\n",
      "(281, 3, 1) (281, 3, 1)\n",
      "(179, 3, 1) (179, 3, 1)\n",
      "(1950, 1, 3, 224, 224)\n",
      "(1950, 3, 224, 224, 1)\n",
      "(406, 3, 1) (406, 3, 1)\n",
      "(195, 3, 1) (195, 3, 1)\n",
      "(2010, 1, 3, 224, 224)\n",
      "(2010, 3, 224, 224, 1)\n",
      "(513, 3, 1) (513, 3, 1)\n",
      "(231, 3, 1) (231, 3, 1)\n",
      "(1755, 1, 3, 224, 224)\n",
      "(1755, 3, 224, 224, 1)\n",
      "(140, 3, 1) (140, 3, 1)\n",
      "(539, 3, 1) (539, 3, 1)\n",
      "(1778, 1, 3, 224, 224)\n",
      "(1778, 3, 224, 224, 1)\n",
      "(142, 3, 1) (142, 3, 1)\n",
      "(396, 3, 1) (396, 3, 1)\n",
      "(1736, 1, 3, 224, 224)\n",
      "(1736, 3, 224, 224, 1)\n",
      "(336, 3, 1) (336, 3, 1)\n",
      "(245, 3, 1) (245, 3, 1)\n",
      "(1820, 1, 3, 224, 224)\n",
      "(1820, 3, 224, 224, 1)\n",
      "(206, 3, 1) (206, 3, 1)\n",
      "(454, 3, 1) (454, 3, 1)\n",
      "(1876, 1, 3, 224, 224)\n",
      "(1876, 3, 224, 224, 1)\n",
      "(467, 3, 1) (467, 3, 1)\n",
      "(242, 3, 1) (242, 3, 1)\n",
      "(1638, 1, 3, 224, 224)\n",
      "(1638, 3, 224, 224, 1)\n",
      "(227, 3, 1) (227, 3, 1)\n",
      "(379, 3, 1) (379, 3, 1)\n",
      "(2032, 1, 4, 224, 224)\n",
      "(2032, 4, 224, 224, 1)\n",
      "(322, 4, 1) (322, 4, 1)\n",
      "(321, 4, 1) (321, 4, 1)\n",
      "(1984, 1, 4, 224, 224)\n",
      "(1984, 4, 224, 224, 1)\n",
      "(186, 4, 1) (186, 4, 1)\n",
      "(520, 4, 1) (520, 4, 1)\n",
      "(2080, 1, 4, 224, 224)\n",
      "(2080, 4, 224, 224, 1)\n",
      "(139, 4, 1) (139, 4, 1)\n",
      "(582, 4, 1) (582, 4, 1)\n",
      "(2144, 1, 4, 224, 224)\n",
      "(2144, 4, 224, 224, 1)\n",
      "(251, 4, 1) (251, 4, 1)\n",
      "(445, 4, 1) (445, 4, 1)\n",
      "(1872, 1, 4, 224, 224)\n",
      "(1872, 4, 224, 224, 1)\n",
      "(202, 4, 1) (202, 4, 1)\n",
      "(462, 4, 1) (462, 4, 1)\n",
      "(1905, 1, 4, 224, 224)\n",
      "(1905, 4, 224, 224, 1)\n",
      "(450, 4, 1) (450, 4, 1)\n",
      "(293, 4, 1) (293, 4, 1)\n",
      "(1860, 1, 4, 224, 224)\n",
      "(1860, 4, 224, 224, 1)\n",
      "(51, 4, 1) (51, 4, 1)\n",
      "(646, 4, 1) (646, 4, 1)\n",
      "(1950, 1, 4, 224, 224)\n",
      "(1950, 4, 224, 224, 1)\n",
      "(216, 4, 1) (216, 4, 1)\n",
      "(440, 4, 1) (440, 4, 1)\n",
      "(2010, 1, 4, 224, 224)\n",
      "(2010, 4, 224, 224, 1)\n",
      "(441, 4, 1) (441, 4, 1)\n",
      "(366, 4, 1) (366, 4, 1)\n",
      "(1755, 1, 4, 224, 224)\n",
      "(1755, 4, 224, 224, 1)\n",
      "(100, 4, 1) (100, 4, 1)\n",
      "(837, 4, 1) (837, 4, 1)\n",
      "(1778, 1, 4, 224, 224)\n",
      "(1778, 4, 224, 224, 1)\n",
      "(196, 4, 1) (196, 4, 1)\n",
      "(363, 4, 1) (363, 4, 1)\n",
      "(1736, 1, 4, 224, 224)\n",
      "(1736, 4, 224, 224, 1)\n",
      "(317, 4, 1) (317, 4, 1)\n",
      "(314, 4, 1) (314, 4, 1)\n",
      "(1820, 1, 4, 224, 224)\n",
      "(1820, 4, 224, 224, 1)\n",
      "(141, 4, 1) (141, 4, 1)\n",
      "(718, 4, 1) (718, 4, 1)\n",
      "(1876, 1, 4, 224, 224)\n",
      "(1876, 4, 224, 224, 1)\n",
      "(0, 4, 1) (0, 4, 1)\n",
      "(72, 4, 1) (72, 4, 1)\n",
      "(1638, 1, 4, 224, 224)\n",
      "(1638, 4, 224, 224, 1)\n",
      "(347, 4, 1) (347, 4, 1)\n",
      "(409, 4, 1) (409, 4, 1)\n",
      "(1651, 1, 4, 224, 224)\n",
      "(1651, 4, 224, 224, 1)\n",
      "(207, 4, 1) (207, 4, 1)\n",
      "(441, 4, 1) (441, 4, 1)\n",
      "(1612, 1, 4, 224, 224)\n",
      "(1612, 4, 224, 224, 1)\n",
      "(210, 4, 1) (210, 4, 1)\n",
      "(165, 4, 1) (165, 4, 1)\n",
      "(1690, 1, 4, 224, 224)\n",
      "(1690, 4, 224, 224, 1)\n",
      "(127, 4, 1) (127, 4, 1)\n",
      "(133, 4, 1) (133, 4, 1)\n",
      "(1742, 1, 4, 224, 224)\n",
      "(1742, 4, 224, 224, 1)\n",
      "(314, 4, 1) (314, 4, 1)\n",
      "(141, 4, 1) (141, 4, 1)\n",
      "(1521, 1, 4, 224, 224)\n",
      "(1521, 4, 224, 224, 1)\n",
      "(202, 4, 1) (202, 4, 1)\n",
      "(204, 4, 1) (204, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Use this code for standard error.\n",
    "\"\"\"\n",
    "#Generate the delay for one model.\n",
    "ch_filter = 193\n",
    "WindNet_attention={}\n",
    "'''\n",
    "    Keys must be of the form attention_type_area_history_delay, where:\n",
    "    type: 'Total', 'Large', or 'Small' for corresponding wind speeds\n",
    "    area: 'hole' or 'hot'\n",
    "'''\n",
    "for history in np.arange(1,5):\n",
    "    for delay in np.arange(1,5):\n",
    "        for cv in np.arange(1,6):\n",
    "            #Get path for the saved model (a.k.a model checkpoint)\n",
    "            GCsavepath='../Models/WindNet/CrossValidation/'+str(ch_filter)+'/CV_'+str(cv)+'/SDOPred'+str(history)+str(delay)+'/'\n",
    "            trainpaths_Mask = sorted(glob('../Data/Mask/CrossValidation'+str(ch_filter)+'/CV_'+str(cv)+'/Train/*.npy'))\n",
    "            GCto=np.load(GCsavepath+'GC_train.npy',allow_pickle=True).tolist()\n",
    "            bp='../Plots/WindNet/'\n",
    "            #Obtain the segmented ARs and CHs for the training data.\n",
    "            Cor_hole=[]\n",
    "            Hot_reg=[]\n",
    "            CH_tmp=[]\n",
    "            Hot_tmp=[]\n",
    "            for fname in trainpaths_Mask:\n",
    "                fopen=np.load(fname,allow_pickle=True).tolist()\n",
    "                CH_tmp.append(np.reshape(fopen['hole'],[-1,224,224,1]))\n",
    "                Hot_tmp.append(np.reshape(fopen['hot'],[-1,224,224,1]))\n",
    "\n",
    "            #We have the masks of format [global_batches,batchsize,224,224,1]. This must be made sequential.\n",
    "            CH_tmp=np.asarray(CH_tmp)\n",
    "            Hot_tmp=np.asarray(Hot_tmp)\n",
    "            for i in xrange(CH_tmp.shape[0]):\n",
    "                for j in xrange(CH_tmp.shape[1]-history-delay+1):\n",
    "                    Cor_hole.append(CH_tmp[i,j:j+history,:])\n",
    "                    Hot_reg.append(Hot_tmp[i,j:j+history,:])\n",
    "            \n",
    "            Masktype={'hole':np.asarray(Cor_hole),'hot':np.asarray(Hot_reg)}\n",
    "\n",
    "            #Made a dictionary for ease of use.\n",
    "            '''\n",
    "                Evaluate the grad cam maps for all values in the training set. Training set is used as it is fitted well (obviously),\n",
    "                and hence gives more confidence to our stochastic result shown later on.\n",
    "            '''\n",
    "            GC_total = GCto['GC']\n",
    "\n",
    "            #IFF necessary, scale the grad cam values here. Else use a max and min across all models.\n",
    "            #GC_total = (GC_total-np.min(GC_total))/(np.max(GC_total)-np.min(GC_total))\n",
    "            print GC_total.shape\n",
    "            print Masktype['hole'].shape\n",
    "            '''\n",
    "                This cell pointewise multiplies the mask with grad cam maps to obtain the contribution of CHs and ARs.\n",
    "            '''\n",
    "            feat_cons='hole'\n",
    "            TotBinMask=Masktype[feat_cons]\n",
    "            GC_total =np.reshape(GC_total,[-1,history,224,224,1])\n",
    "            GC_CoronalHole_contrib = TotBinMask*GC_total\n",
    "            feat_cons='hot'\n",
    "            TotBinMask=Masktype[feat_cons]\n",
    "            GC_total =np.reshape(GC_total,[-1,history,224,224,1])\n",
    "            GC_hot_contrib = TotBinMask*GC_total\n",
    "            GC_CoronalHole_contrib_mn = np.mean(GC_CoronalHole_contrib,axis=(2,3))\n",
    "            GC_CoronalHole_std = np.mean(np.square(GC_CoronalHole_contrib-np.reshape(GC_CoronalHole_contrib_mn,[-1,history,1,1,1])),axis=(0,2,3))\n",
    "            GC_CoronalHole_std=np.sqrt(GC_CoronalHole_std)\n",
    "            GC_hot_contrib_mn = np.mean(GC_hot_contrib,axis=(2,3))\n",
    "            GC_hot_std = np.mean(np.square(GC_hot_contrib-np.reshape(GC_hot_contrib_mn,[-1,history,1,1,1])),axis=(0,2,3))\n",
    "            GC_hot_std=np.sqrt(GC_hot_std)\n",
    "            WindNet_attention['attention_total_hole_'+str(history)+'_'+str(delay)+'_'+str(cv)] = (GC_CoronalHole_contrib_mn,GC_CoronalHole_std)\n",
    "            WindNet_attention['attention_total_hot_'+str(history)+'_'+str(delay)+'_'+str(cv)] = (GC_hot_contrib_mn,GC_hot_std)\n",
    "            #WindNet_attention['mask_total_hot_'+str(history)+'_'+str(delay)] = Masktype['hot']\n",
    "            #WindNet_attention['mask_total_hole_'+str(history)+'_'+str(delay)] = np.sum(Masktype['hole'],axis=(2,3))\n",
    "            '''\n",
    "                Defining:\n",
    "                    Large SW: vel>500\n",
    "                    Small SW: vel<=350\n",
    "                Pickout the indices where the predicted solar wind velocity is high and low. \n",
    "            '''\n",
    "            Inds = GCto['IND']\n",
    "            WindNet_attention['n_indices_%d_%d_%d'%(history,delay,cv)] = GC_total.shape[0]\n",
    "            for swtype in Inds.keys():\n",
    "                ind_list=Inds[swtype]\n",
    "                feat_cons='hole'\n",
    "                TotBinMask=Masktype[feat_cons][ind_list,:]\n",
    "                GC_total =np.reshape(GC_total,[-1,history,224,224,1])\n",
    "                GC_CoronalHole_contrib = TotBinMask*(GC_total[ind_list,:])\n",
    "                feat_cons='hot'\n",
    "                TotBinMask=Masktype[feat_cons][ind_list,:]\n",
    "                GC_total =np.reshape(GC_total,[-1,history,224,224,1])\n",
    "                GC_hot_contrib = TotBinMask*(GC_total[ind_list,:])\n",
    "                GC_CoronalHole_contrib_mn = np.mean(GC_CoronalHole_contrib,axis=(2,3))\n",
    "                GC_CoronalHole_std = np.mean(np.square(GC_CoronalHole_contrib-np.reshape(GC_CoronalHole_contrib_mn,[-1,history,1,1,1])),axis=(2,3))\n",
    "                GC_hot_contrib_mn = np.mean(GC_hot_contrib,axis=(2,3))\n",
    "                GC_hot_std = np.mean(np.square(GC_hot_contrib-np.reshape(GC_hot_contrib_mn,[-1,history,1,1,1])),axis=(2,3))\n",
    "                GC_CoronalHole_std=np.sqrt(GC_CoronalHole_std)\n",
    "                GC_hot_std=np.sqrt(GC_hot_std)\n",
    "                print GC_CoronalHole_contrib_mn.shape,GC_hot_contrib_mn.shape\n",
    "                WindNet_attention['attention_'+str(swtype)+'_hole_'+str(history)+'_'+str(delay)+'_'+str(cv)] = (GC_CoronalHole_contrib_mn,GC_CoronalHole_std)\n",
    "                WindNet_attention['attention_'+str(swtype)+'_hot_'+str(history)+'_'+str(delay)+'_'+str(cv)] = (GC_hot_contrib_mn,GC_hot_std)\n",
    "            np.save(bp+'WindNet_attn_'+str(ch_filter),WindNet_attention)\n",
    "            #or k in WindNet_attention.keys():\n",
    "                #rint WindNet_attention[k].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attention_Large_closedLoop_1_1_1', 'attention_Large_closedLoop_1_1_2', 'attention_Large_closedLoop_1_1_3', 'attention_Large_closedLoop_1_1_4', 'attention_Large_closedLoop_1_1_5', 'attention_Large_closedLoop_1_2_1', 'attention_Large_closedLoop_1_2_2', 'attention_Large_closedLoop_1_2_3', 'attention_Large_closedLoop_1_2_4', 'attention_Large_closedLoop_1_2_5', 'attention_Large_closedLoop_1_3_1', 'attention_Large_closedLoop_1_3_2', 'attention_Large_closedLoop_1_3_3', 'attention_Large_closedLoop_1_3_4', 'attention_Large_closedLoop_1_3_5', 'attention_Large_closedLoop_1_4_1', 'attention_Large_closedLoop_1_4_2', 'attention_Large_closedLoop_1_4_3', 'attention_Large_closedLoop_1_4_4', 'attention_Large_closedLoop_1_4_5', 'attention_Large_closedLoop_2_1_1', 'attention_Large_closedLoop_2_1_2', 'attention_Large_closedLoop_2_1_3', 'attention_Large_closedLoop_2_1_4', 'attention_Large_closedLoop_2_1_5', 'attention_Large_closedLoop_2_2_1', 'attention_Large_closedLoop_2_2_2', 'attention_Large_closedLoop_2_2_3', 'attention_Large_closedLoop_2_2_4', 'attention_Large_closedLoop_2_2_5', 'attention_Large_closedLoop_2_3_1', 'attention_Large_closedLoop_2_3_2', 'attention_Large_closedLoop_2_3_3', 'attention_Large_closedLoop_2_3_4', 'attention_Large_closedLoop_2_3_5', 'attention_Large_closedLoop_2_4_1', 'attention_Large_closedLoop_2_4_2', 'attention_Large_closedLoop_2_4_3', 'attention_Large_closedLoop_2_4_4', 'attention_Large_closedLoop_2_4_5', 'attention_Large_closedLoop_3_1_1', 'attention_Large_closedLoop_3_1_2', 'attention_Large_closedLoop_3_1_3', 'attention_Large_closedLoop_3_1_4', 'attention_Large_closedLoop_3_1_5', 'attention_Large_closedLoop_3_2_1', 'attention_Large_closedLoop_3_2_2', 'attention_Large_closedLoop_3_2_3', 'attention_Large_closedLoop_3_2_4', 'attention_Large_closedLoop_3_2_5', 'attention_Large_closedLoop_3_3_1', 'attention_Large_closedLoop_3_3_2', 'attention_Large_closedLoop_3_3_3', 'attention_Large_closedLoop_3_3_4', 'attention_Large_closedLoop_3_3_5', 'attention_Large_closedLoop_3_4_1', 'attention_Large_closedLoop_3_4_2', 'attention_Large_closedLoop_3_4_3', 'attention_Large_closedLoop_3_4_4', 'attention_Large_closedLoop_3_4_5', 'attention_Large_closedLoop_4_1_1', 'attention_Large_closedLoop_4_1_2', 'attention_Large_closedLoop_4_1_3', 'attention_Large_closedLoop_4_1_4', 'attention_Large_closedLoop_4_1_5', 'attention_Large_closedLoop_4_2_1', 'attention_Large_closedLoop_4_2_2', 'attention_Large_closedLoop_4_2_3', 'attention_Large_closedLoop_4_2_4', 'attention_Large_closedLoop_4_2_5', 'attention_Large_closedLoop_4_3_1', 'attention_Large_closedLoop_4_3_2', 'attention_Large_closedLoop_4_3_3', 'attention_Large_closedLoop_4_3_4', 'attention_Large_closedLoop_4_3_5', 'attention_Large_closedLoop_4_4_1', 'attention_Large_closedLoop_4_4_2', 'attention_Large_closedLoop_4_4_3', 'attention_Large_closedLoop_4_4_4', 'attention_Large_closedLoop_4_4_5', 'attention_Large_hole_1_1_1', 'attention_Large_hole_1_1_2', 'attention_Large_hole_1_1_3', 'attention_Large_hole_1_1_4', 'attention_Large_hole_1_1_5', 'attention_Large_hole_1_2_1', 'attention_Large_hole_1_2_2', 'attention_Large_hole_1_2_3', 'attention_Large_hole_1_2_4', 'attention_Large_hole_1_2_5', 'attention_Large_hole_1_3_1', 'attention_Large_hole_1_3_2', 'attention_Large_hole_1_3_3', 'attention_Large_hole_1_3_4', 'attention_Large_hole_1_3_5', 'attention_Large_hole_1_4_1', 'attention_Large_hole_1_4_2', 'attention_Large_hole_1_4_3', 'attention_Large_hole_1_4_4', 'attention_Large_hole_1_4_5', 'attention_Large_hole_2_1_1', 'attention_Large_hole_2_1_2', 'attention_Large_hole_2_1_3', 'attention_Large_hole_2_1_4', 'attention_Large_hole_2_1_5', 'attention_Large_hole_2_2_1', 'attention_Large_hole_2_2_2', 'attention_Large_hole_2_2_3', 'attention_Large_hole_2_2_4', 'attention_Large_hole_2_2_5', 'attention_Large_hole_2_3_1', 'attention_Large_hole_2_3_2', 'attention_Large_hole_2_3_3', 'attention_Large_hole_2_3_4', 'attention_Large_hole_2_3_5', 'attention_Large_hole_2_4_1', 'attention_Large_hole_2_4_2', 'attention_Large_hole_2_4_3', 'attention_Large_hole_2_4_4', 'attention_Large_hole_2_4_5', 'attention_Large_hole_3_1_1', 'attention_Large_hole_3_1_2', 'attention_Large_hole_3_1_3', 'attention_Large_hole_3_1_4', 'attention_Large_hole_3_1_5', 'attention_Large_hole_3_2_1', 'attention_Large_hole_3_2_2', 'attention_Large_hole_3_2_3', 'attention_Large_hole_3_2_4', 'attention_Large_hole_3_2_5', 'attention_Large_hole_3_3_1', 'attention_Large_hole_3_3_2', 'attention_Large_hole_3_3_3', 'attention_Large_hole_3_3_4', 'attention_Large_hole_3_3_5', 'attention_Large_hole_3_4_1', 'attention_Large_hole_3_4_2', 'attention_Large_hole_3_4_3', 'attention_Large_hole_3_4_4', 'attention_Large_hole_3_4_5', 'attention_Large_hole_4_1_1', 'attention_Large_hole_4_1_2', 'attention_Large_hole_4_1_3', 'attention_Large_hole_4_1_4', 'attention_Large_hole_4_1_5', 'attention_Large_hole_4_2_1', 'attention_Large_hole_4_2_2', 'attention_Large_hole_4_2_3', 'attention_Large_hole_4_2_4', 'attention_Large_hole_4_2_5', 'attention_Large_hole_4_3_1', 'attention_Large_hole_4_3_2', 'attention_Large_hole_4_3_3', 'attention_Large_hole_4_3_4', 'attention_Large_hole_4_3_5', 'attention_Large_hole_4_4_1', 'attention_Large_hole_4_4_2', 'attention_Large_hole_4_4_3', 'attention_Large_hole_4_4_4', 'attention_Large_hole_4_4_5', 'attention_Large_hot_1_1_1', 'attention_Large_hot_1_1_2', 'attention_Large_hot_1_1_3', 'attention_Large_hot_1_1_4', 'attention_Large_hot_1_1_5', 'attention_Large_hot_1_2_1', 'attention_Large_hot_1_2_2', 'attention_Large_hot_1_2_3', 'attention_Large_hot_1_2_4', 'attention_Large_hot_1_2_5', 'attention_Large_hot_1_3_1', 'attention_Large_hot_1_3_2', 'attention_Large_hot_1_3_3', 'attention_Large_hot_1_3_4', 'attention_Large_hot_1_3_5', 'attention_Large_hot_1_4_1', 'attention_Large_hot_1_4_2', 'attention_Large_hot_1_4_3', 'attention_Large_hot_1_4_4', 'attention_Large_hot_1_4_5', 'attention_Large_hot_2_1_1', 'attention_Large_hot_2_1_2', 'attention_Large_hot_2_1_3', 'attention_Large_hot_2_1_4', 'attention_Large_hot_2_1_5', 'attention_Large_hot_2_2_1', 'attention_Large_hot_2_2_2', 'attention_Large_hot_2_2_3', 'attention_Large_hot_2_2_4', 'attention_Large_hot_2_2_5', 'attention_Large_hot_2_3_1', 'attention_Large_hot_2_3_2', 'attention_Large_hot_2_3_3', 'attention_Large_hot_2_3_4', 'attention_Large_hot_2_3_5', 'attention_Large_hot_2_4_1', 'attention_Large_hot_2_4_2', 'attention_Large_hot_2_4_3', 'attention_Large_hot_2_4_4', 'attention_Large_hot_2_4_5', 'attention_Large_hot_3_1_1', 'attention_Large_hot_3_1_2', 'attention_Large_hot_3_1_3', 'attention_Large_hot_3_1_4', 'attention_Large_hot_3_1_5', 'attention_Large_hot_3_2_1', 'attention_Large_hot_3_2_2', 'attention_Large_hot_3_2_3', 'attention_Large_hot_3_2_4', 'attention_Large_hot_3_2_5', 'attention_Large_hot_3_3_1', 'attention_Large_hot_3_3_2', 'attention_Large_hot_3_3_3', 'attention_Large_hot_3_3_4', 'attention_Large_hot_3_3_5', 'attention_Large_hot_3_4_1', 'attention_Large_hot_3_4_2', 'attention_Large_hot_3_4_3', 'attention_Large_hot_3_4_4', 'attention_Large_hot_3_4_5', 'attention_Large_hot_4_1_1', 'attention_Large_hot_4_1_2', 'attention_Large_hot_4_1_3', 'attention_Large_hot_4_1_4', 'attention_Large_hot_4_1_5', 'attention_Large_hot_4_2_1', 'attention_Large_hot_4_2_2', 'attention_Large_hot_4_2_3', 'attention_Large_hot_4_2_4', 'attention_Large_hot_4_2_5', 'attention_Large_hot_4_3_1', 'attention_Large_hot_4_3_2', 'attention_Large_hot_4_3_3', 'attention_Large_hot_4_3_4', 'attention_Large_hot_4_3_5', 'attention_Large_hot_4_4_1', 'attention_Large_hot_4_4_2', 'attention_Large_hot_4_4_3', 'attention_Large_hot_4_4_4', 'attention_Large_hot_4_4_5', 'attention_Small_closedLoop_1_1_1', 'attention_Small_closedLoop_1_1_2', 'attention_Small_closedLoop_1_1_3', 'attention_Small_closedLoop_1_1_4', 'attention_Small_closedLoop_1_1_5', 'attention_Small_closedLoop_1_2_1', 'attention_Small_closedLoop_1_2_2', 'attention_Small_closedLoop_1_2_3', 'attention_Small_closedLoop_1_2_4', 'attention_Small_closedLoop_1_2_5', 'attention_Small_closedLoop_1_3_1', 'attention_Small_closedLoop_1_3_2', 'attention_Small_closedLoop_1_3_3', 'attention_Small_closedLoop_1_3_4', 'attention_Small_closedLoop_1_3_5', 'attention_Small_closedLoop_1_4_1', 'attention_Small_closedLoop_1_4_2', 'attention_Small_closedLoop_1_4_3', 'attention_Small_closedLoop_1_4_4', 'attention_Small_closedLoop_1_4_5', 'attention_Small_closedLoop_2_1_1', 'attention_Small_closedLoop_2_1_2', 'attention_Small_closedLoop_2_1_3', 'attention_Small_closedLoop_2_1_4', 'attention_Small_closedLoop_2_1_5', 'attention_Small_closedLoop_2_2_1', 'attention_Small_closedLoop_2_2_2', 'attention_Small_closedLoop_2_2_3', 'attention_Small_closedLoop_2_2_4', 'attention_Small_closedLoop_2_2_5', 'attention_Small_closedLoop_2_3_1', 'attention_Small_closedLoop_2_3_2', 'attention_Small_closedLoop_2_3_3', 'attention_Small_closedLoop_2_3_4', 'attention_Small_closedLoop_2_3_5', 'attention_Small_closedLoop_2_4_1', 'attention_Small_closedLoop_2_4_2', 'attention_Small_closedLoop_2_4_3', 'attention_Small_closedLoop_2_4_4', 'attention_Small_closedLoop_2_4_5', 'attention_Small_closedLoop_3_1_1', 'attention_Small_closedLoop_3_1_2', 'attention_Small_closedLoop_3_1_3', 'attention_Small_closedLoop_3_1_4', 'attention_Small_closedLoop_3_1_5', 'attention_Small_closedLoop_3_2_1', 'attention_Small_closedLoop_3_2_2', 'attention_Small_closedLoop_3_2_3', 'attention_Small_closedLoop_3_2_4', 'attention_Small_closedLoop_3_2_5', 'attention_Small_closedLoop_3_3_1', 'attention_Small_closedLoop_3_3_2', 'attention_Small_closedLoop_3_3_3', 'attention_Small_closedLoop_3_3_4', 'attention_Small_closedLoop_3_3_5', 'attention_Small_closedLoop_3_4_1', 'attention_Small_closedLoop_3_4_2', 'attention_Small_closedLoop_3_4_3', 'attention_Small_closedLoop_3_4_4', 'attention_Small_closedLoop_3_4_5', 'attention_Small_closedLoop_4_1_1', 'attention_Small_closedLoop_4_1_2', 'attention_Small_closedLoop_4_1_3', 'attention_Small_closedLoop_4_1_4', 'attention_Small_closedLoop_4_1_5', 'attention_Small_closedLoop_4_2_1', 'attention_Small_closedLoop_4_2_2', 'attention_Small_closedLoop_4_2_3', 'attention_Small_closedLoop_4_2_4', 'attention_Small_closedLoop_4_2_5', 'attention_Small_closedLoop_4_3_1', 'attention_Small_closedLoop_4_3_2', 'attention_Small_closedLoop_4_3_3', 'attention_Small_closedLoop_4_3_4', 'attention_Small_closedLoop_4_3_5', 'attention_Small_closedLoop_4_4_1', 'attention_Small_closedLoop_4_4_2', 'attention_Small_closedLoop_4_4_3', 'attention_Small_closedLoop_4_4_4', 'attention_Small_closedLoop_4_4_5', 'attention_Small_hole_1_1_1', 'attention_Small_hole_1_1_2', 'attention_Small_hole_1_1_3', 'attention_Small_hole_1_1_4', 'attention_Small_hole_1_1_5', 'attention_Small_hole_1_2_1', 'attention_Small_hole_1_2_2', 'attention_Small_hole_1_2_3', 'attention_Small_hole_1_2_4', 'attention_Small_hole_1_2_5', 'attention_Small_hole_1_3_1', 'attention_Small_hole_1_3_2', 'attention_Small_hole_1_3_3', 'attention_Small_hole_1_3_4', 'attention_Small_hole_1_3_5', 'attention_Small_hole_1_4_1', 'attention_Small_hole_1_4_2', 'attention_Small_hole_1_4_3', 'attention_Small_hole_1_4_4', 'attention_Small_hole_1_4_5', 'attention_Small_hole_2_1_1', 'attention_Small_hole_2_1_2', 'attention_Small_hole_2_1_3', 'attention_Small_hole_2_1_4', 'attention_Small_hole_2_1_5', 'attention_Small_hole_2_2_1', 'attention_Small_hole_2_2_2', 'attention_Small_hole_2_2_3', 'attention_Small_hole_2_2_4', 'attention_Small_hole_2_2_5', 'attention_Small_hole_2_3_1', 'attention_Small_hole_2_3_2', 'attention_Small_hole_2_3_3', 'attention_Small_hole_2_3_4', 'attention_Small_hole_2_3_5', 'attention_Small_hole_2_4_1', 'attention_Small_hole_2_4_2', 'attention_Small_hole_2_4_3', 'attention_Small_hole_2_4_4', 'attention_Small_hole_2_4_5', 'attention_Small_hole_3_1_1', 'attention_Small_hole_3_1_2', 'attention_Small_hole_3_1_3', 'attention_Small_hole_3_1_4', 'attention_Small_hole_3_1_5', 'attention_Small_hole_3_2_1', 'attention_Small_hole_3_2_2', 'attention_Small_hole_3_2_3', 'attention_Small_hole_3_2_4', 'attention_Small_hole_3_2_5', 'attention_Small_hole_3_3_1', 'attention_Small_hole_3_3_2', 'attention_Small_hole_3_3_3', 'attention_Small_hole_3_3_4', 'attention_Small_hole_3_3_5', 'attention_Small_hole_3_4_1', 'attention_Small_hole_3_4_2', 'attention_Small_hole_3_4_3', 'attention_Small_hole_3_4_4', 'attention_Small_hole_3_4_5', 'attention_Small_hole_4_1_1', 'attention_Small_hole_4_1_2', 'attention_Small_hole_4_1_3', 'attention_Small_hole_4_1_4', 'attention_Small_hole_4_1_5', 'attention_Small_hole_4_2_1', 'attention_Small_hole_4_2_2', 'attention_Small_hole_4_2_3', 'attention_Small_hole_4_2_4', 'attention_Small_hole_4_2_5', 'attention_Small_hole_4_3_1', 'attention_Small_hole_4_3_2', 'attention_Small_hole_4_3_3', 'attention_Small_hole_4_3_4', 'attention_Small_hole_4_3_5', 'attention_Small_hole_4_4_1', 'attention_Small_hole_4_4_2', 'attention_Small_hole_4_4_3', 'attention_Small_hole_4_4_4', 'attention_Small_hole_4_4_5', 'attention_Small_hot_1_1_1', 'attention_Small_hot_1_1_2', 'attention_Small_hot_1_1_3', 'attention_Small_hot_1_1_4', 'attention_Small_hot_1_1_5', 'attention_Small_hot_1_2_1', 'attention_Small_hot_1_2_2', 'attention_Small_hot_1_2_3', 'attention_Small_hot_1_2_4', 'attention_Small_hot_1_2_5', 'attention_Small_hot_1_3_1', 'attention_Small_hot_1_3_2', 'attention_Small_hot_1_3_3', 'attention_Small_hot_1_3_4', 'attention_Small_hot_1_3_5', 'attention_Small_hot_1_4_1', 'attention_Small_hot_1_4_2', 'attention_Small_hot_1_4_3', 'attention_Small_hot_1_4_4', 'attention_Small_hot_1_4_5', 'attention_Small_hot_2_1_1', 'attention_Small_hot_2_1_2', 'attention_Small_hot_2_1_3', 'attention_Small_hot_2_1_4', 'attention_Small_hot_2_1_5', 'attention_Small_hot_2_2_1', 'attention_Small_hot_2_2_2', 'attention_Small_hot_2_2_3', 'attention_Small_hot_2_2_4', 'attention_Small_hot_2_2_5', 'attention_Small_hot_2_3_1', 'attention_Small_hot_2_3_2', 'attention_Small_hot_2_3_3', 'attention_Small_hot_2_3_4', 'attention_Small_hot_2_3_5', 'attention_Small_hot_2_4_1', 'attention_Small_hot_2_4_2', 'attention_Small_hot_2_4_3', 'attention_Small_hot_2_4_4', 'attention_Small_hot_2_4_5', 'attention_Small_hot_3_1_1', 'attention_Small_hot_3_1_2', 'attention_Small_hot_3_1_3', 'attention_Small_hot_3_1_4', 'attention_Small_hot_3_1_5', 'attention_Small_hot_3_2_1', 'attention_Small_hot_3_2_2', 'attention_Small_hot_3_2_3', 'attention_Small_hot_3_2_4', 'attention_Small_hot_3_2_5', 'attention_Small_hot_3_3_1', 'attention_Small_hot_3_3_2', 'attention_Small_hot_3_3_3', 'attention_Small_hot_3_3_4', 'attention_Small_hot_3_3_5', 'attention_Small_hot_3_4_1', 'attention_Small_hot_3_4_2', 'attention_Small_hot_3_4_3', 'attention_Small_hot_3_4_4', 'attention_Small_hot_3_4_5', 'attention_Small_hot_4_1_1', 'attention_Small_hot_4_1_2', 'attention_Small_hot_4_1_3', 'attention_Small_hot_4_1_4', 'attention_Small_hot_4_1_5', 'attention_Small_hot_4_2_1', 'attention_Small_hot_4_2_2', 'attention_Small_hot_4_2_3', 'attention_Small_hot_4_2_4', 'attention_Small_hot_4_2_5', 'attention_Small_hot_4_3_1', 'attention_Small_hot_4_3_2', 'attention_Small_hot_4_3_3', 'attention_Small_hot_4_3_4', 'attention_Small_hot_4_3_5', 'attention_Small_hot_4_4_1', 'attention_Small_hot_4_4_2', 'attention_Small_hot_4_4_3', 'attention_Small_hot_4_4_4', 'attention_Small_hot_4_4_5', 'attention_total_closedLoop_1_1_1', 'attention_total_closedLoop_1_1_2', 'attention_total_closedLoop_1_1_3', 'attention_total_closedLoop_1_1_4', 'attention_total_closedLoop_1_1_5', 'attention_total_closedLoop_1_2_1', 'attention_total_closedLoop_1_2_2', 'attention_total_closedLoop_1_2_3', 'attention_total_closedLoop_1_2_4', 'attention_total_closedLoop_1_2_5', 'attention_total_closedLoop_1_3_1', 'attention_total_closedLoop_1_3_2', 'attention_total_closedLoop_1_3_3', 'attention_total_closedLoop_1_3_4', 'attention_total_closedLoop_1_3_5', 'attention_total_closedLoop_1_4_1', 'attention_total_closedLoop_1_4_2', 'attention_total_closedLoop_1_4_3', 'attention_total_closedLoop_1_4_4', 'attention_total_closedLoop_1_4_5', 'attention_total_closedLoop_2_1_1', 'attention_total_closedLoop_2_1_2', 'attention_total_closedLoop_2_1_3', 'attention_total_closedLoop_2_1_4', 'attention_total_closedLoop_2_1_5', 'attention_total_closedLoop_2_2_1', 'attention_total_closedLoop_2_2_2', 'attention_total_closedLoop_2_2_3', 'attention_total_closedLoop_2_2_4', 'attention_total_closedLoop_2_2_5', 'attention_total_closedLoop_2_3_1', 'attention_total_closedLoop_2_3_2', 'attention_total_closedLoop_2_3_3', 'attention_total_closedLoop_2_3_4', 'attention_total_closedLoop_2_3_5', 'attention_total_closedLoop_2_4_1', 'attention_total_closedLoop_2_4_2', 'attention_total_closedLoop_2_4_3', 'attention_total_closedLoop_2_4_4', 'attention_total_closedLoop_2_4_5', 'attention_total_closedLoop_3_1_1', 'attention_total_closedLoop_3_1_2', 'attention_total_closedLoop_3_1_3', 'attention_total_closedLoop_3_1_4', 'attention_total_closedLoop_3_1_5', 'attention_total_closedLoop_3_2_1', 'attention_total_closedLoop_3_2_2', 'attention_total_closedLoop_3_2_3', 'attention_total_closedLoop_3_2_4', 'attention_total_closedLoop_3_2_5', 'attention_total_closedLoop_3_3_1', 'attention_total_closedLoop_3_3_2', 'attention_total_closedLoop_3_3_3', 'attention_total_closedLoop_3_3_4', 'attention_total_closedLoop_3_3_5', 'attention_total_closedLoop_3_4_1', 'attention_total_closedLoop_3_4_2', 'attention_total_closedLoop_3_4_3', 'attention_total_closedLoop_3_4_4', 'attention_total_closedLoop_3_4_5', 'attention_total_closedLoop_4_1_1', 'attention_total_closedLoop_4_1_2', 'attention_total_closedLoop_4_1_3', 'attention_total_closedLoop_4_1_4', 'attention_total_closedLoop_4_1_5', 'attention_total_closedLoop_4_2_1', 'attention_total_closedLoop_4_2_2', 'attention_total_closedLoop_4_2_3', 'attention_total_closedLoop_4_2_4', 'attention_total_closedLoop_4_2_5', 'attention_total_closedLoop_4_3_1', 'attention_total_closedLoop_4_3_2', 'attention_total_closedLoop_4_3_3', 'attention_total_closedLoop_4_3_4', 'attention_total_closedLoop_4_3_5', 'attention_total_closedLoop_4_4_1', 'attention_total_closedLoop_4_4_2', 'attention_total_closedLoop_4_4_3', 'attention_total_closedLoop_4_4_4', 'attention_total_closedLoop_4_4_5', 'attention_total_hole_1_1_1', 'attention_total_hole_1_1_2', 'attention_total_hole_1_1_3', 'attention_total_hole_1_1_4', 'attention_total_hole_1_1_5', 'attention_total_hole_1_2_1', 'attention_total_hole_1_2_2', 'attention_total_hole_1_2_3', 'attention_total_hole_1_2_4', 'attention_total_hole_1_2_5', 'attention_total_hole_1_3_1', 'attention_total_hole_1_3_2', 'attention_total_hole_1_3_3', 'attention_total_hole_1_3_4', 'attention_total_hole_1_3_5', 'attention_total_hole_1_4_1', 'attention_total_hole_1_4_2', 'attention_total_hole_1_4_3', 'attention_total_hole_1_4_4', 'attention_total_hole_1_4_5', 'attention_total_hole_2_1_1', 'attention_total_hole_2_1_2', 'attention_total_hole_2_1_3', 'attention_total_hole_2_1_4', 'attention_total_hole_2_1_5', 'attention_total_hole_2_2_1', 'attention_total_hole_2_2_2', 'attention_total_hole_2_2_3', 'attention_total_hole_2_2_4', 'attention_total_hole_2_2_5', 'attention_total_hole_2_3_1', 'attention_total_hole_2_3_2', 'attention_total_hole_2_3_3', 'attention_total_hole_2_3_4', 'attention_total_hole_2_3_5', 'attention_total_hole_2_4_1', 'attention_total_hole_2_4_2', 'attention_total_hole_2_4_3', 'attention_total_hole_2_4_4', 'attention_total_hole_2_4_5', 'attention_total_hole_3_1_1', 'attention_total_hole_3_1_2', 'attention_total_hole_3_1_3', 'attention_total_hole_3_1_4', 'attention_total_hole_3_1_5', 'attention_total_hole_3_2_1', 'attention_total_hole_3_2_2', 'attention_total_hole_3_2_3', 'attention_total_hole_3_2_4', 'attention_total_hole_3_2_5', 'attention_total_hole_3_3_1', 'attention_total_hole_3_3_2', 'attention_total_hole_3_3_3', 'attention_total_hole_3_3_4', 'attention_total_hole_3_3_5', 'attention_total_hole_3_4_1', 'attention_total_hole_3_4_2', 'attention_total_hole_3_4_3', 'attention_total_hole_3_4_4', 'attention_total_hole_3_4_5', 'attention_total_hole_4_1_1', 'attention_total_hole_4_1_2', 'attention_total_hole_4_1_3', 'attention_total_hole_4_1_4', 'attention_total_hole_4_1_5', 'attention_total_hole_4_2_1', 'attention_total_hole_4_2_2', 'attention_total_hole_4_2_3', 'attention_total_hole_4_2_4', 'attention_total_hole_4_2_5', 'attention_total_hole_4_3_1', 'attention_total_hole_4_3_2', 'attention_total_hole_4_3_3', 'attention_total_hole_4_3_4', 'attention_total_hole_4_3_5', 'attention_total_hole_4_4_1', 'attention_total_hole_4_4_2', 'attention_total_hole_4_4_3', 'attention_total_hole_4_4_4', 'attention_total_hole_4_4_5', 'attention_total_hot_1_1_1', 'attention_total_hot_1_1_2', 'attention_total_hot_1_1_3', 'attention_total_hot_1_1_4', 'attention_total_hot_1_1_5', 'attention_total_hot_1_2_1', 'attention_total_hot_1_2_2', 'attention_total_hot_1_2_3', 'attention_total_hot_1_2_4', 'attention_total_hot_1_2_5', 'attention_total_hot_1_3_1', 'attention_total_hot_1_3_2', 'attention_total_hot_1_3_3', 'attention_total_hot_1_3_4', 'attention_total_hot_1_3_5', 'attention_total_hot_1_4_1', 'attention_total_hot_1_4_2', 'attention_total_hot_1_4_3', 'attention_total_hot_1_4_4', 'attention_total_hot_1_4_5', 'attention_total_hot_2_1_1', 'attention_total_hot_2_1_2', 'attention_total_hot_2_1_3', 'attention_total_hot_2_1_4', 'attention_total_hot_2_1_5', 'attention_total_hot_2_2_1', 'attention_total_hot_2_2_2', 'attention_total_hot_2_2_3', 'attention_total_hot_2_2_4', 'attention_total_hot_2_2_5', 'attention_total_hot_2_3_1', 'attention_total_hot_2_3_2', 'attention_total_hot_2_3_3', 'attention_total_hot_2_3_4', 'attention_total_hot_2_3_5', 'attention_total_hot_2_4_1', 'attention_total_hot_2_4_2', 'attention_total_hot_2_4_3', 'attention_total_hot_2_4_4', 'attention_total_hot_2_4_5', 'attention_total_hot_3_1_1', 'attention_total_hot_3_1_2', 'attention_total_hot_3_1_3', 'attention_total_hot_3_1_4', 'attention_total_hot_3_1_5', 'attention_total_hot_3_2_1', 'attention_total_hot_3_2_2', 'attention_total_hot_3_2_3', 'attention_total_hot_3_2_4', 'attention_total_hot_3_2_5', 'attention_total_hot_3_3_1', 'attention_total_hot_3_3_2', 'attention_total_hot_3_3_3', 'attention_total_hot_3_3_4', 'attention_total_hot_3_3_5', 'attention_total_hot_3_4_1', 'attention_total_hot_3_4_2', 'attention_total_hot_3_4_3', 'attention_total_hot_3_4_4', 'attention_total_hot_3_4_5', 'attention_total_hot_4_1_1', 'attention_total_hot_4_1_2', 'attention_total_hot_4_1_3', 'attention_total_hot_4_1_4', 'attention_total_hot_4_1_5', 'attention_total_hot_4_2_1', 'attention_total_hot_4_2_2', 'attention_total_hot_4_2_3', 'attention_total_hot_4_2_4', 'attention_total_hot_4_2_5', 'attention_total_hot_4_3_1', 'attention_total_hot_4_3_2', 'attention_total_hot_4_3_3', 'attention_total_hot_4_3_4', 'attention_total_hot_4_3_5', 'attention_total_hot_4_4_1', 'attention_total_hot_4_4_2', 'attention_total_hot_4_4_3', 'attention_total_hot_4_4_4', 'attention_total_hot_4_4_5']\n"
     ]
    }
   ],
   "source": [
    "print sorted(WindNet_attention.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the activation plot\n",
    "\n",
    "Now, we have the activation values with $\\sigma$ ready. Go ahead and run the notebook `MakePlots.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
